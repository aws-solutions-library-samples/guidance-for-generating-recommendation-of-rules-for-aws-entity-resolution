{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"# GenAI based rule recommendations for AWS Entity Resolution\n",
				"This notebook will walk you through the process of building a GenAI based rule recommendation for [AWS Entity Resolution](https://aws.amazon.com/entity-resolution/) using a Large Language Model (LLM) hosted on [Amazon Bedrock](https://aws.amazon.com/bedrock/). We will use an existing AWS Entity Resolution [Schema Mapping](https://docs.aws.amazon.com/entityresolution/latest/userguide/schema-mapping.html) as a source. We will generate data quality metrics of the source data and provide that as an input to the prompt for the LLM model. The LLM model will provide a rule recommendation based on the input prompt with its reasoning."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"<div class=\"alert alert-block alert-info\">\n",
				"<b>Pre-requisites for this notebook:</b>\n",
				"    <ul>\n",
				"        <li>IAM Role:\n",
				"            <ul>\n",
				"                <li>Access to AWS Entity Resolution</li>\n",
				"                <li>Access to invoke the Foundation Models on Amazon Bedrock</li>\n",
				"                <li>Read access to Amazon S3 source data bucket</li>\n",
				"                <li><i>Optional:</i>\n",
				"                    <ul><li>If you want to access <a href=\"https://docs.aws.amazon.com/codewhisperer/latest/userguide/what-is-cwspr.html\">Amazon Code Whisperer</a> to get machine learning-powered code generator that provides you with code recommendations, use this <a href='https://docs.aws.amazon.com/codewhisperer/latest/userguide/glue-setup.html'>link</a> to setup.</li></ul>\n",
				"            </ul>\n",
				"        </li>\n",
				"        <li>This notebook is recommended to be run with <i>Glue 5.0</i> and boto3 version <i>1.34.131</i> or above</li>\n",
				"        <li>At the time of writing this notebook, Amazon Bedrock was only available in <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html#bedrock-regions\">these supported AWS Regions</a>. If you are running this notebook from any other AWS Region, then you have to change the Amazon Bedrock client's region and/or endpoint URL parameters to one of those supported AWS Regions. Follow the guidance in the <i>Organize imports</i> section of this notebook.</li>        \n",
				"    </ul>\n",
				"</div>"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Setting the PySpark session"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%additional_python_modules networkx\n",
				"%idle_timeout 90\n",
				"%glue_version 5.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 5"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"spark"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"**Table of Contents:**\n",
				"\n",
				"1. [Pre-requisites](#1)\n",
				"\n",
				"    1.a. [Set AWS Region and boto3 config](#1.a)\n",
				"  \n",
				"    1.b. [Organize imports](#1.b)\n",
				"       \n",
				"    1.c. [Enable model access in Amazon Bedrock](#1.c)\n",
				"    \n",
				"    1.d. [Check and configure security permissions](#1.d)\n",
				"    \n",
				" 2. [Data Preparation](#2)\n",
				" \n",
				"    2.a. [Fetch the AWS Entity Resolution schema mapping for your input dataset](#2.a)        \n",
				"        \n",
				"    2.b. [Start the interactive PySpark session](#2.b)\n",
				"    \n",
				"    2.c. [Read and prepare input data from the AWS Glue table defined in the AWS Glue Data Catalog](#2.c)\n",
				"    \n",
				" 3. [Generate Data Quality metrics](#3)\n",
				"    \n",
				"    3.a. [Calculate the percentage distribution of empty/missing values for every column](#3.a)\n",
				"    \n",
				"    3.b. [Calculate the frequency distribution of Top 3 values for every column](#3.b)\n",
				"    \n",
				" 4. [Invoke the LLM Model](#4)\n",
				"    \n",
				"    4.a. [Provide the model id of the LLM model to use with Amazon Bedrock](#4.a)    \n",
				"\n",
				"    4.b. [Prepare the LLM prompt for getting the rule recommendation](#4.b)\n",
				"    \n",
				"    4.c. [Invoke the LLM model for getting the rule recommendation](#4.c)\n",
				"    \n",
				"    4.d. [Extract and parse the rule generated by LLM](#4.d)\n",
				"    \n",
				"    4.e. [Validate the rule on input dataset](#4.e)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 1. Pre-requisites <a id='1'></a>"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### 1.a AWS Region and boto3 config <a id=\"1.a\"></a>\n",
				"Get the current AWS Region (where this notebook is running). This will be used to initiate some of the clients to AWS services using the boto3 APIs."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"<div class=\"alert alert-block alert-info\">\n",
				"    <b>Note:</b> All the AWS services used by this notebook except Amazon Bedrock will use the current AWS Region. For Bedrock, follow the guidance in the next section.\n",
				"</div>"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"<div class=\"alert alert-block alert-warning\">  \n",
				"<b>Note:</b> At the time of writing this notebook, Amazon Bedrock was only available in <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html#bedrock-regions\">these supported AWS Regions</a>. If you are running this notebook from any other AWS Region, then you have to change the Amazon Bedrock client's region and/or endpoint URL parameters to one of those supported AWS Regions. In order to do this, this notebook will use the value specified in the environment variable named <mark>AMAZON_BEDROCK_REGION</mark>. If this is not specified, then the notebook will default to <mark>us-east-1 (N. Virginia)</mark> for Amazon Bedrock.\n",
				"</div>\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"import boto3\n",
				"from botocore.config import Config\n",
				"import os\n",
				"\n",
				"print(\"boto3 version: {} and expected is 1.34.131 or above\".format(boto3.__version__))\n",
				"\n",
				"my_session = boto3.session.Session()\n",
				"\n",
				"my_region = my_session.region_name\n",
				"print(\"Current AWS Region: {}\".format(my_region))\n",
				"\n",
				"# Explicity set the AWS Region for Amazon Bedrock clients\n",
				"AMAZON_BEDROCK_DEFAULT_REGION = \"us-east-1\"\n",
				"br_region = os.environ.get('AMAZON_BEDROCK_REGION')\n",
				"\n",
				"if br_region is None:\n",
				"    br_region = AMAZON_BEDROCK_DEFAULT_REGION\n",
				"elif len(br_region) == 0:\n",
				"    br_region = AMAZON_BEDROCK_DEFAULT_REGION\n",
				"print(\"AWS Region for Amazon Bedrock: {}\".format(br_region))"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"Set the timeout and retry configurations that will be applied to all the boto3 clients used in this notebook."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Increase the standard time out limits in the boto3 client from 1 minute to 3 minutes\n",
				"# and set the retry limits\n",
				"my_boto3_config = Config(\n",
				"    connect_timeout = (60 * 3),\n",
				"    read_timeout = (60 * 3),\n",
				"    retries = {\n",
				"        'max_attempts': 600,\n",
				"        'mode': 'adaptive'\n",
				"    }\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### 1.b Organize imports <a id='1.b'></a>"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"\n",
				"from pyspark.sql import DataFrame\n",
				"from pyspark.sql.functions import concat,concat_ws, col, lit, trim,lower,isnan,when,count,round\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### 1.c Enable model access in Amazon Bedrock <a id ='1.c'> </a>"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"<div class=\"alert alert-block alert-info\">\n",
				"    <b>Note:</b> Before invoking any model in Amazon Bedrock, enable access to that model by following the instructions <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\">here</a>. In addition, for Anthropic models, you need to submit the use case details. Otherwise, you will get an authorization error.\n",
				"    <br /><br />\n",
				"    By default, this notebook is expecting access to <b>Claude 3.5 Sonnet v2</b> model. Please access request to the model using the below link if you do not have it already.\n",
				"</div>"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"Run the following cell to print the Amazon Bedrock model access page URL for the AWS Region that was selected earlier."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Print the Amazon Bedrock model access page URL\n",
				"print(\"Amazon Bedrock model access page - https://{}.console.aws.amazon.com/bedrock/home?region={}#/modelaccess\"\n",
				"             .format(br_region, br_region))"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 2. Data Preparation <a id=\"2\" />"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### 2.a Fetch the AWS Entity Resolution schema mapping for your input dataset <a id='2.a'></a>"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"<div class=\"alert alert-info alert-block\"><b>Note: </b>Update the AWS Glue Database, AWS Glue Table, and the AWS Entity Resolution schema mapping name based on your environment</div>"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"schemaName = 'sourceSchemaMapping'\n",
				"awsGlueDatabase = 'entityresolution'\n",
				"awsGlueTable = 'sourceGlueTable'"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"aerClient = boto3.client('entityresolution')\n",
				"\n",
				"#fetch the schema mapping definition specified above \n",
				"aerSchemaResponse = aerClient.get_schema_mapping(\n",
				"    schemaName=schemaName\n",
				")\n",
				"print(aerSchemaResponse)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Parse the schema mapping definition and extract the relevant fields, their types and the group definitions"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"schemaFieldList = aerSchemaResponse['mappedInputFields']\n",
				"\n",
				"# a static ordering of how the Name group is created within AWS Entity Resolution\n",
				"nameOrder = ['NAME_FIRST','NAME_MIDDLE','NAME_LAST']\n",
				"\n",
				"# a static ordering of how the Address group is created within AWS Entity Resolution\n",
				"addressOrder = ['ADDRESS_STREET1','ADDRESS_STREET2','ADDRESS_STREET3','ADDRESS_CITY','ADDRESS_STATE','ADDRESS_POSTALCODE','ADDRESS_COUNTRY']\n",
				"\n",
				"addressDict = []\n",
				"nameDict = []\n",
				"fields = []\n",
				"uniqueIdColumn = ''\n",
				"\n",
				"addressGroupName = ''\n",
				"nameGroupName = ''\n",
				"\n",
				"for field in schemaFieldList:\n",
				"    \n",
				"    if(field[\"type\"] == 'UNIQUE_ID'):\n",
				"        uniqueIdColumn = field['fieldName']\n",
				"        fields.append(field['fieldName'])\n",
				"    \n",
				"    if('matchKey' in field):\n",
				"        fields.append(field['fieldName'])\n",
				"\n",
				"        if(\"groupName\" in field):\n",
				"            if(field['matchKey'] == 'Address'):\n",
				"                addressGroupName = field['groupName']\n",
				"                addressDict.append(field)\n",
				"                fields.remove(field['fieldName'])\n",
				"            if(field['matchKey'] == 'Name'):\n",
				"                nameGroupName = field['groupName']        \n",
				"                nameDict.append(field)\n",
				"                fields.remove(field['fieldName'])\n",
				"\n",
				"def orderedFieldListForGroup(typeList,typeDict):\n",
				"    finalList = []\n",
				"    for field in typeList:\n",
				"        for field2 in typeDict:\n",
				"            if(field2['type'] == field):\n",
				"                finalList.append(field2['fieldName'])\n",
				"    return finalList\n",
				"\n",
				"addressList = orderedFieldListForGroup(addressOrder, addressDict)\n",
				"nameList = orderedFieldListForGroup(nameOrder, nameDict)\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### 2.b Start the PySpark interactive session <a id='2.b'></a>"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"#### 2.c Read and prepare the input data from the AWS Glue table defined in the AWS Glue Data Catalog <a id=\"2.c\" />\n",
				"\n",
				"The next few cells read the input data using AWS Glue Dynamic frame, and convert it into Spark DataFrame. It applies transformations to create the final DataFrame representing the shape of the schema as defined in the AWS Entity Resolution schema mapping.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"dyf = glueContext.create_dynamic_frame.from_catalog(database=awsGlueDatabase, table_name=awsGlueTable)\n",
				"dyf.printSchema()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"#### Convert the DynamicFrame to a Spark DataFrame and display a sample of the data\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"df = dyf.toDF()\n",
				"df.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Only select the columns that are part of the schema mapping definition and create the concatenated group based on the schema definition"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql.functions import concat,concat_ws, col, lit, trim,lower,isnan,when,count,round\n",
				"\n",
				"filteredDF = df.select(*fields,*addressList, *nameList)\n",
				"\n",
				"if(addressGroupName!=''):\n",
				"    filteredDF = filteredDF.withColumn(addressGroupName,concat_ws(\" \",*addressList)).drop(*addressList)\n",
				"    \n",
				"if(nameGroupName!=''):    \n",
				"    filteredDF = filteredDF.withColumn(nameGroupName,concat_ws(\" \",*nameList)).drop(*nameList)\n",
				"\n",
				"\n",
				"def trim_all_string_columns(df: DataFrame) -> DataFrame:\n",
				"    return df\\\n",
				"        .select(\n",
				"            *[trim(col(c[0])).alias(c[0]) if c[1] == 'string' else col(c[0]) for c in df.dtypes]\n",
				"        )\n",
				"\n",
				"def lowercase_all_string_columns(df: DataFrame) -> DataFrame:\n",
				"    return df\\\n",
				"        .select(\n",
				"            *[lower(col(c[0])).alias(c[0]) if c[1] == 'string' else col(c[0]) for c in df.dtypes]\n",
				"        )\n",
				"\n",
				"    \n",
				"\n",
				"#mimicing AWS Entity Resolution normalization by converting every string column to lower case and removing all whitespaces\n",
				"filteredDF = filteredDF.transform(lowercase_all_string_columns)\n",
				"#filteredDF = filteredDF.transform(trim_all_string_columns)\n",
				"\n",
				"\n",
				"filteredDF.printSchema()\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### View sample records"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"filteredDF.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"## 3. Generate Data Quality metrics <a id=\"3\" />\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Calculate the total number of records in the input dataset"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"recordCount = filteredDF.count()\n",
				"print(recordCount)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### 3.a Calculate the percentage distribution of empty/missing values for every column <a id=\"3.a\" />"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql.functions import col, lit, trim,lower,isnan,when,count,round\n",
				"emptyValuesDF = filteredDF.select([round((count(when(col(c).contains('None') | \\\n",
				"                            col(c).contains('NULL') | \\\n",
				"                            (col(c) == '' ) | \\\n",
				"                            col(c).isNull() | \\\n",
				"                            isnan(c), c \n",
				"                           ))/recordCount*100),2).alias(c)\n",
				"                    for c in filteredDF.columns])\n",
				"\n",
				"emptyValuesDict = dict()\n",
				"for column in emptyValuesDF.columns:\n",
				"    emptyValuesDict[column] = emptyValuesDF.select(column).first()[0]\n",
				"    \n",
				"emptyValuesDF.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Draw a graph of the percentage distribution"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"import seaborn as sns\n",
				"import matplotlib.pyplot as plt\n",
				"\n",
				"#sns.set_theme(style=\"whitegrid\")\n",
				"\n",
				"p = emptyValuesDF.toPandas()\n",
				"fig, ax = plt.subplots(figsize=(8, 8))\n",
				"\n",
				"emptyC = emptyValuesDF.first()\n",
				"\n",
				"colList = emptyValuesDF.columns\n",
				"emptyList = list(emptyC.asDict().values())\n",
				"sns.barplot(data=emptyValuesDF.toPandas(), palette=\"ch:start=.2,rot=-.3\")\n",
				"\n",
				"plt.xticks(rotation=90)\n",
				"ax.set_ylabel('percentage')\n",
				"ax.set_title('Empty Value percentage')\n",
				"plt.rcParams.update({'font.size':12})\n",
				"fig.subplots_adjust(bottom=0.2) \n",
				"\n",
				"%matplot plt"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### 3.b Calculate the frequency distribution of Top 3 values for every column <a id=\"3.b\" />\n",
				"Calculate the frequency/occurences of the top 3 values configured by the variable `noOfTop`. The output contains the percentage value of the occurence in the whole dataset."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql.functions import (abs as df_abs, col, count, countDistinct,\n",
				"                                   max as df_max, mean, min as df_min,\n",
				"                                   sum as df_sum, when\n",
				"                                   )\n",
				"\n",
				"# the frequency of the top occuring words\n",
				"noOfTop = 3\n",
				"\n",
				"def guess_json_type(string_value):\n",
				"        try:\n",
				"            obj = json.loads(string_value)\n",
				"        except:\n",
				"            return None\n",
				"\n",
				"        return type(obj)\n",
				"    \n",
				"def describe_categorical_1d(df, column, recordCount):\n",
				"        count_column_name = \"count({c})\".format(c=column)\n",
				"\n",
				"        value_counts = (df.select(column).na.drop()\n",
				"                        .groupBy(column)\n",
				"                        .agg(count(col(column)))\n",
				"                        .orderBy(count_column_name, ascending=False)\n",
				"                       ).cache()\n",
				"\n",
				"        # Get the noOfTop classes by value count,\n",
				"        # and put the rest of them grouped at the\n",
				"        # end of the Series:\n",
				"        top_Freq = value_counts.limit(noOfTop).toPandas().sort_values(count_column_name,\n",
				"                                                               ascending=False)\n",
				"        \n",
				"        top_Freq[count_column_name] = top_Freq[count_column_name]/recordCount*100\n",
				"        top_Freq[count_column_name] = top_Freq[count_column_name].round(2).astype(str) + '%'\n",
				"        \n",
				"\n",
				"        \n",
				"        top_Freq.rename(columns={count_column_name: \"frequency\"})\n",
				"        \n",
				"        stats = top_Freq.take([0]).rename(columns={column: 'top', count_column_name: 'freq'}).iloc[0]\n",
				"        \n",
				"        others_count = 0\n",
				"        others_distinct_count = 0\n",
				"        unique_categories_count = value_counts.count()\n",
				"        \n",
				"        value_counts.unpersist()\n",
				"        top = top_Freq.set_index(column)[count_column_name]\n",
				"        stats[\"value_counts\"] = top\n",
				"        stats[\"unique values\"] = unique_categories_count\n",
				"        return top.to_json(orient=\"index\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql import functions as f\n",
				"frequencyDict = dict()\n",
				"for colname in filteredDF.columns:\n",
				"    frequencyDict[colname] = describe_categorical_1d(filteredDF, colname, recordCount)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"print(frequencyDict)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 4. Invoke the LLM Model <a id=\"4\" />"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### 4.a Provide the model id of the LLM model to use with Amazon Bedrock <a id=\"4.a\" />\n",
				"<div class=\"alert alert-info alert-block\"><b>Note: </b>Refer to this <a href='https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html'>link</a> to get the model id of the LLM. In certain cases, the model id may refer to an ARN, as is the case for Anthropic's Claude 3.5 Sonnet v2.<br /><br />Update the <i>region</i> and <i>AWSAccountNumber</i> in the below cell.</div>"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"model_id = 'arn:aws:bedrock:[region]:[AWSAccountNumber]:inference-profile/us.anthropic.claude-3-5-sonnet-20241022-v2:0'"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### 4.b Prepare the prompt for the LLM model to generate the rule recommendation <a id=\"4.b\" />"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"The LLM model uses a few shot learning to make the LLM model aware of the constraints of the AWS Entity Resolution matching. This is to ensure that the output generated by the model is not generic, but specific to the capabilities of the service."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"prompt = f\"\"\"Entity resolution is the process of determining when multiple records belong to the same person, despite differences in how they are described or inconsistencies in how data was entered.\n",
				"\n",
				"As an example, these 2 records belong to the same person:\n",
				"Record 1:\n",
				"Name: Jon Doe II\n",
				"Address: 123 East Main St.\n",
				"Phone: +1(897)777-1414\n",
				"Email: jondoe@gmail.com\n",
				"\n",
				"Record 2:\n",
				"Name: Jonathan Doe Jr.\n",
				"Address: 123 East Main St.\n",
				"Phone: 897.777.1414\n",
				"Email:\n",
				"\n",
				"The conclusion is that the above two records belong to the same person because they both share similar names, Jonathan and Jon being common alias/nickname, and share the same address and phone number even though the second record is missing the email identifier. A rule that can be applied deterministically to confirm this would be: (name) AND (Address Or Email Or Phone)\n",
				"\n",
				"Entity resolution can also determine when multiple records do not belong to the same person. As an example if the name in one record contains Sr. as the suffix, while the other record contains Jr. as the suffix, it can be determined that the records belong to two different individuals despite they sharing the same address and phone, primarily because it would indicate a father and a son living in the same house.\n",
				"\n",
				"While determinig the rule, it is important to understand the quality of the input records. If any column has a reasonable percentage of missing values, the rule should either consider skipping that column or use it in a \"OR\" condition with another column. The rule should try to consider all the input columns.\n",
				"\n",
				"Using this information, generate a rule that can be applied to determine if the records belong to the same person. The rule should be deterministic, with no consideration for fuzzy or geo-mapping. Also provide the reasoning for those rules, and no code snippets are required. If you ignore any of the input column, explain why in your reasoning. \n",
				"\n",
				"Data Quality for the input dataset is defined for each of the input column. The json for each column lists the top 3 occurences and their frequency from 0 to 100%. For any column, if the empty value is very high, you may ignore that column in the rule. If for any column, a particular value has high frequency occurence, there is a possibility that it may result in poor accuracy result.\"\"\"\n",
				"\n",
				"for column in emptyValuesDict:\n",
				"    prompt += f\"\"\"\\n\n",
				"Column: {column}\n",
				"Empty Value: {emptyValuesDict[column]}\n",
				"Frequency: {frequencyDict[column]}\n",
				"\n",
				"The generated rule should only choose the operator from [ExactMatch, AND, OR], where ExactMatch(column) means the values in that column need to match exactly. Output your rules after \"Recommended Rule:\"\n",
				"If possible, make the rule simple and easy to understand.\n",
				"\"\"\"\n",
				"\n",
				"print(prompt)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### 4.c Invoke the LLM model with the given prompt <a id=\"4.c\" />"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"import json\n",
				"from botocore.exceptions import ClientError\n",
				"bedrockClient = boto3.client(\"bedrock-runtime\")\n",
				"\n",
				"# Define the prompt for the model.\n",
				"\n",
				"accept = 'application/json'\n",
				"contentType = 'application/json'\n",
				"\n",
				"# Format the request payload using the model's native structure.\n",
				"native_request = {\n",
				"    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
				"    \"max_tokens\": 2000,\n",
				"    \"temperature\": 1,\n",
				"    \"messages\": [\n",
				"        {\n",
				"            \"role\": \"user\",\n",
				"            \"content\": [{\"type\": \"text\", \"text\": prompt}],\n",
				"        }\n",
				"    ]\n",
				"}\n",
				"\n",
				"# Convert the native request to JSON.\n",
				"request = json.dumps(native_request)\n",
				"\n",
				"try:\n",
				"    # Invoke the model with the request.\n",
				"    response = bedrockClient.invoke_model(modelId=model_id, body=request)\n",
				"\n",
				"except (Exception) as e:\n",
				"    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
				"    exit(1)\n",
				"\n",
				"# Decode the response body.\n",
				"\n",
				"\n",
				"\n",
				"model_response = json.loads(response[\"body\"].read())\n",
				"\n",
				"# Extract and print the response text.\n",
				"response_text = model_response[\"content\"][0][\"text\"]\n",
				"print(response_text)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### 4.d Extract and parse the rule suggested by the LLM <a id=\"4.d\" />"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"generated_rule = \"\"\n",
				"try:\n",
				"    generated_rule = response_text.split(\"Recommended Rule:\")[1].split('\\n\\n')[0].replace('\\n', '').strip()\n",
				"except Exception as e:\n",
				"    print(\"An error occurred:\", e)\n",
				"    \n",
				"generated_rule"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql import functions as F\n",
				"import re\n",
				"\n",
				"def parse_rule_to_condition(rule_str, alias1=\"df1\", alias2=\"df2\"):\n",
				"    # Replace 'ExactMatch(col)' with Spark's NULL-safe equality check\n",
				"    rule_str = re.sub(\n",
				"        r'ExactMatch\\((\\w+)\\)',\n",
				"        lambda m: f'(F.col(\"{alias1}.{m.group(1)}\").eqNullSafe(F.col(\"{alias2}.{m.group(1)}\")))',\n",
				"        rule_str\n",
				"    )\n",
				"    # Replace logical operators with PySpark-compatible symbols\n",
				"    rule_str = rule_str.replace(\"AND\", \"&\").replace(\"OR\", \"|\")\n",
				"    # Evaluate the string into a PySpark Column condition\n",
				"    return eval(rule_str)\n",
				"\n",
				"df1 = filteredDF.alias(\"df1\")\n",
				"df2 = filteredDF.alias(\"df2\")\n",
				"condition = parse_rule_to_condition(generated_rule, \"df1\", \"df2\")\n",
				"print(condition)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### 4.e Validate the rule on the input dataset <a id=\"4.e\" />\n",
				"Apply the rule on the input dataset to look for pairs that match due to the rule"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"colNameA = \"df1.\"+uniqueIdColumn\n",
				"colNameB = \"df2.\"+uniqueIdColumn\n",
				"\n",
				"condition = condition & (F.col(colNameA) < F.col(colNameB))\n",
				"\n",
				"found_pairs = df1.join(df2, condition).select(\n",
				"    F.col(colNameA).alias(\"id1\"),\n",
				"    F.col(colNameB).alias(\"id2\")\n",
				")\n",
				"\n",
				"print(f\"this rule found {found_pairs.count()} matches\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"found_pairs.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Draw a connected graph with sample records"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"pd = found_pairs.limit(17).toPandas()\n",
				"print(pd)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"import networkx as nx\n",
				"import matplotlib.pyplot as plt\n",
				"\n",
				"\n",
				"\n",
				"plt.clf()\n",
				"G = nx.DiGraph()\n",
				"\n",
				"G.add_nodes_from(pd['id1'])\n",
				"G.add_nodes_from(pd['id2'])\n",
				"edges = [(row['id1'], row['id2']) for index, row in pd.iterrows()]\n",
				"G.add_edges_from(edges)\n",
				"\n",
				"\n",
				"pos = nx.planar_layout(G)\n",
				"\n",
				"options = {\n",
				"    \"font_size\": 8,\n",
				"    \"node_size\": 1000,\n",
				"    \"node_color\": \"skyblue\",\n",
				"    \"edgecolors\": \"black\",\n",
				"    \"linewidths\": 1,\n",
				"    \"width\": 1,\n",
				"}\n",
				"nx.draw_networkx(G, pos, **options)\n",
				"\n",
				"# Set margins for the axes so that nodes aren't clipped\n",
				"ax = plt.gca()\n",
				"ax.margins(0.01)\n",
				"plt.axis(\"off\")\n",
				"\n",
				"%matplot plt"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Apply a random sampling to get matched paris and verify the result"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"#randomly sample one pair for quality checking\n",
				"\n",
				"sampled_pair = found_pairs.orderBy(F.rand()).limit(1).first()\n",
				"\n",
				"id1 = sampled_pair.id1\n",
				"id2 = sampled_pair.id2\n",
				"print(f\"generated rule :\\n{generated_rule}\")\n",
				"print(f\"sample pair: id1 = {id1}, id2 = {id2}\")\n",
				"row1 = filteredDF.filter(F.col(uniqueIdColumn) == id1).first()\n",
				"row2 = filteredDF.filter(F.col(uniqueIdColumn) == id2).first()\n",
				"\n",
				"for col in filteredDF.columns:\n",
				"    val1 = row1[col] if row1[col] is not None else \"NULL\"\n",
				"    val2 = row2[col] if row2[col] is not None else \"NULL\"\n",
				"    match = \"Match\" if val1 == val2 else \"No-Match\"\n",
				"    print(f\"| {col:20} | {match:10} | {str(val1):20} | {str(val2):20} |\")\n",
				"    "
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
